{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "#import os\n",
    "from plyfile import PlyData, PlyElement\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pptk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import laspy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "import open3d as o3d\n",
    "#import pyntcloud\n",
    "import pandas as pd\n",
    "#from pyntcloud import Voxelize\n",
    "from scipy.spatial import cKDTree\n",
    "import time\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\"\"\"\n",
    "parser.add_argument('--segment_point_size', type=int, default=131072, help='resample points number') #Choose here number of point resample\n",
    "parser.add_argument('--dataset_type', type=str, choices = ('train', 'test'), default='train', help='if train, dataset just splited, downsampled, normalized, if test same, but also gives numbers to reconstruct and paint dataset later')\n",
    "parser.add_argument('--output_dataset_folder', type=str, default=r'nuevaparticion/test/', help='train datasetfolder') #las_test\n",
    "\"\"\"\n",
    "\n",
    "#parser.add_argument('--original_las', type=str, default=r'raws/train/', help='train datasetfolder') #las_test\n",
    "#parser.add_argument('--dataset_type', type=str, choices = ('train', 'test'), default='train', help='if train, dataset just splited, downsampled, normalized, if test same, but also gives numbers to reconstruct and paint dataset later')\n",
    "#opt = parser.parse_args()\n",
    "\n",
    "\n",
    "#opt.original_las\n",
    "\n",
    "def split_dataset(output_folder, opt_original_las):\n",
    "    print(\"Procesando nubes...\")\n",
    "\n",
    "    # Ruta de la carpeta para guardar los segmentos\n",
    "    #output_folder = opt.output_dataset_folder\n",
    "\n",
    "    # Crear la carpeta si no existe\n",
    "    #os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in tqdm(os.listdir(opt_original_las), desc =\"Conjunto de Todas las nubes:\"):\n",
    "        f = os.path.join(opt_original_las, filename)\n",
    "        if os.path.isfile(f):\n",
    "\n",
    "            print(f)\n",
    "\n",
    "            plydata = PlyData.read(f) #print(plydata)\n",
    "\n",
    "            x = plydata[\"vertex\"].data[\"x\"].astype(np.float32)\n",
    "            y = plydata[\"vertex\"].data[\"y\"].astype(np.float32)\n",
    "            z = plydata[\"vertex\"].data[\"z\"].astype(np.float32)\n",
    "\n",
    "            point_data = np.stack([x, y, z], axis=0)\n",
    "\n",
    "            #ESTO SI HAY QUE RESTAR 1 A LAS CLASESS\n",
    "            #adapta_clases = las.classification\n",
    "            #adapta_clases = np.subtract(adapta_clases, 1)\n",
    "\n",
    "            reflectance = plydata[\"vertex\"].data[\"reflectance\"].astype(np.float32)\n",
    "\n",
    "            label = plydata[\"vertex\"].data[\"class\"].astype(np.float32) #label solo si es train\n",
    "            \n",
    "            #true_label = plydata[\"vertex\"].data[\"label\"].astype(np.float32) #label solo si es train\n",
    "            #reflectance = label = plydata[\"vertex\"].data[\"reflectance\"].astype(np.float32) #label solo si es train\n",
    "\n",
    "            xyz = np.array([x,y,z]).T\n",
    "            \n",
    "            xyzic = np.stack([x, y, z, reflectance , label], axis=0).transpose((1, 0))\n",
    "            print(xyzic.shape)\n",
    "\n",
    "\n",
    "            ###################\n",
    "\n",
    "            x_min = np.min(xyzic[:, 0])\n",
    "            x_max = np.max(xyzic[:, 0])\n",
    "            y_min = np.min(xyzic[:, 1])\n",
    "            y_max = np.max(xyzic[:, 1])\n",
    "\n",
    "            divide_x = np.ceil((x_max - x_min) / 51).astype(int)\n",
    "            divide_y = np.ceil((y_max - y_min) / 51).astype(int)\n",
    "\n",
    "            count = 0\n",
    "            for i in range(divide_x):\n",
    "                for j in range(divide_y):\n",
    "                    # Definir lÃ­mites de segmento\n",
    "                    x_min_segment = x_min + i * 51\n",
    "                    x_max_segment = x_min + (i + 1) * 51\n",
    "                    y_min_segment = y_min + j * 51\n",
    "                    y_max_segment = y_min + (j + 1) * 51\n",
    "\n",
    "                    # Filtrar puntos dentro del segmento\n",
    "                    segment_points = xyzic[\n",
    "                        (xyzic[:, 0] >= x_min_segment)\n",
    "                        & (xyzic[:, 0] < x_max_segment)\n",
    "                        & (xyzic[:, 1] >= y_min_segment)\n",
    "                        & (xyzic[:, 1] < y_max_segment)\n",
    "                    ]\n",
    "                    count += 1\n",
    "\n",
    "                    filename_seg = os.path.splitext(filename)[0]+f\"_{str(count).zfill(3)}\"\n",
    "                    #filename = f\"segment_{i}_{j}.npy\"\n",
    "\n",
    "                    filepath = os.path.join(output_folder, filename_seg)\n",
    "                    if len(segment_points)>1:\n",
    "                        np.save(filepath, segment_points)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_original_las = r'C:/Users/sfernandez/nueva_etapa/github2/LidarSegmentationPytorch/Datasets/ParisLille/training_10_classes'\n",
    "output_folder = r'train/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "split_dataset(output_folder, opt_original_las)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHORA A PROCESAR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
